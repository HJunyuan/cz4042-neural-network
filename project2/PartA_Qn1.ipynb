{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "id": "5Ym8hIuzRrfA",
    "outputId": "df92c6cf-47dc-49d3-ca5b-74cab2993a6d"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Project 2, starter code Part a\n",
    "#\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TANmKrj_RrfG"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "IMG_SIZE = 32\n",
    "NUM_CHANNELS = 3\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "batch_size = 128\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CuqIa1_uRrfI"
   },
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        try:\n",
    "            samples = pickle.load(fo)\n",
    "        except UnicodeDecodeError:  #python 3.x\n",
    "            fo.seek(0)\n",
    "            samples = pickle.load(fo, encoding='latin1')\n",
    "\n",
    "    data, labels = samples['data'], samples['labels']\n",
    "\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "\n",
    "    labels_ = np.zeros([labels.shape[0], NUM_CLASSES])\n",
    "    labels_[np.arange(labels.shape[0]), labels-1] = 1\n",
    "\n",
    "    return data, labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MReWO-qyRrfL"
   },
   "outputs": [],
   "source": [
    "def cnn(images):\n",
    "    images = tf.reshape(images, [-1, IMG_SIZE, IMG_SIZE, NUM_CHANNELS])\n",
    "\n",
    "    # C1 (50x24x24): Conv layer, 50 filters, window size 9x9, VALID padding, ReLU\n",
    "    # S1 (50x12x12): Max pooling layer, window size 2x2, stride = 2, VALID padding\n",
    "    W1 = tf.Variable(tf.truncated_normal([9, 9, NUM_CHANNELS, 50], stddev=1.0/np.sqrt(NUM_CHANNELS*9*9)), name='weights_1')\n",
    "    b1 = tf.Variable(tf.zeros([50]), name='biases_1')  \n",
    "    conv_1 = tf.nn.relu(tf.nn.conv2d(images, W1, [1, 1, 1, 1], padding='VALID') + b1)\n",
    "    pool_1 = tf.nn.max_pool(conv_1, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_1')\n",
    "\n",
    "    # C2 (60x8x8): Conv layer, 60 filters, window size 5x5, VALID padding, ReLU\n",
    "    # S2 (60x4x4): Max pooling layer, window size 2x2, stride = 2, VALID padding\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, 50, 60], stddev=1.0/np.sqrt(50*5*5)), name='weights_2')\n",
    "    b2 = tf.Variable(tf.zeros([60]), name='biases_2')\n",
    "    conv_2 = tf.nn.relu(tf.nn.conv2d(pool_1, W2, [1, 1, 1, 1], padding='VALID') + b2)\n",
    "    pool_2 = tf.nn.max_pool(conv_2, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_2')\n",
    "\n",
    "    # Flatten (dim = 60x4x4 = 960)\n",
    "    dim = pool_2.get_shape()[1].value * pool_2.get_shape()[2].value * pool_2.get_shape()[3].value \n",
    "    pool_2_flat = tf.reshape(pool_2, [-1, dim])\n",
    "\n",
    "    # F3: Fully connected layer of size 300 (960 -> 300)\n",
    "    W3 = tf.Variable(tf.truncated_normal([dim, 300], stddev=1.0/np.sqrt(dim)), name='weights_3')\n",
    "    b3 = tf.Variable(tf.zeros([300]), name='biases_3')\n",
    "    f3_logits = tf.matmul(pool_2_flat, W3) + b3\n",
    "\n",
    "    # F4: Softmax layer of size 10 (300 -> 10)\n",
    "    W4 = tf.Variable(tf.truncated_normal([300, NUM_CLASSES], stddev=1.0/np.sqrt(300)), name='weights_4')\n",
    "    b4 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_4')\n",
    "    f4_logits = tf.matmul(f3_logits, W4) + b4\n",
    "\n",
    "    return conv_1, pool_1, conv_2, pool_2, f4_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cbVhB4GYOAp9",
    "outputId": "f189b5fe-dc2b-4ef3-f93c-da55659e7013"
   },
   "outputs": [],
   "source": [
    "# Main Program\n",
    "if __name__ == '__main__':\n",
    "    trainX, trainY = load_data('/content/drive/My Drive/NTU - Year 3 Sem 1/CZ4042 Neural Network/Project 2/data_batch_1')\n",
    "    print(trainX.shape, trainY.shape)\n",
    "\n",
    "    testX, testY = load_data('/content/drive/My Drive/NTU - Year 3 Sem 1/CZ4042 Neural Network/Project 2/test_batch_trim')\n",
    "    print(testX.shape, testY.shape)\n",
    "\n",
    "    # Scaling the train & test inputs\n",
    "    trainX = (trainX - np.min(trainX, axis = 0))/np.max(trainX, axis = 0)\n",
    "    testX = (testX - np.min(testX, axis = 0))/np.max(testX, axis = 0)\n",
    "\n",
    "    # Create the model\n",
    "    x = tf.placeholder(tf.float32, [None, NUM_CHANNELS*IMG_SIZE*IMG_SIZE]) # 3x32x32\n",
    "    y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "    c1, s1, c2, s2, logits = cnn(x)\n",
    "\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "    N = len(trainX)\n",
    "    idx = np.arange(N)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "\n",
    "        for e in range(epochs):\n",
    "            np.random.shuffle(idx)\n",
    "            trainX, trainY = trainX[idx], trainY[idx]\n",
    "            temp_train_loss = []\n",
    "\n",
    "            for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):\n",
    "                train_step.run(feed_dict={x: trainX[start:end], y_: trainY[start:end]})\n",
    "                temp_train_loss.append(loss.eval(feed_dict={x: trainX[start:end], y_: trainY[start:end]}))\n",
    "\n",
    "            # _, loss_ = sess.run([train_step, loss], {x: trainX, y_: trainY})\n",
    "\n",
    "            trainLoss = np.mean(np.array(temp_train_loss))\n",
    "            testAcc = accuracy.eval(feed_dict={x: testX, y_: testY})\n",
    "            train_loss.append(trainLoss)\n",
    "            test_acc.append(testAcc)\n",
    "\n",
    "            if (e % 10 == 0 or e == epochs-1):\n",
    "                print('Epoch: ', e, ' | Train loss: ', trainLoss, ' | Test acc: ', testAcc)\n",
    "\n",
    "        # Plot train loss vs test acc\n",
    "        plt.figure(figsize=(10, 8), dpi= 100)\n",
    "        plt.suptitle('Train loss and test accuracies')\n",
    "        plt.plot(range(epochs), train_loss, label='Train Loss')\n",
    "        plt.plot(range(epochs), test_acc, label='Test Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy/loss')\n",
    "        plt.savefig('./1a.TrainLoss_TestAcc.png')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Plots\n",
    "        for i in range(2):\n",
    "            # ind = np.random.randint(low=0, high=10000)\n",
    "\n",
    "            # Test Patterns\n",
    "            plt.figure()\n",
    "            plt.suptitle('Test Pattern %d' %(i+1))\n",
    "            plt.gray()\n",
    "            plt.axis('off')\n",
    "            X = testX[i,:]\n",
    "            X_show = X.reshape(NUM_CHANNELS, IMG_SIZE, IMG_SIZE).transpose(1, 2, 0)\n",
    "            plt.imshow(X_show)\n",
    "            plt.savefig('./1b.TP%d_Original.png' %(i+1))\n",
    "\n",
    "            # Feature Maps\n",
    "            def plotFeatureMap(layer, tpNum, layerText):\n",
    "                plt.figure()\n",
    "                plt.suptitle('Test Pattern {}: Feature Map {}'.format(tpNum+1, layerText))\n",
    "                plt.gray()\n",
    "                layer_show = layer[0].transpose(2, 0, 1)\n",
    "                for j in range(len(layer_show)):\n",
    "                    if (len(layer_show) == 50):\n",
    "                        plt.subplot(10, 5, j+1)\n",
    "                    elif (len(layer_show) == 60):\n",
    "                        plt.subplot(10, 6, j+1)\n",
    "                    plt.axis('off')\n",
    "                    plt.imshow(layer_show[j])\n",
    "                plt.savefig('./1b.TP{}_FM_{}.png'.format(tpNum+1, layerText))\n",
    "\n",
    "            c1_, s1_, c2_, s2_ = sess.run([c1, s1, c2, s2],{x: testX[i,:].reshape(1,3072)})\n",
    "            plotFeatureMap(c1_, i, 'C1')\n",
    "            plotFeatureMap(s1_, i, 'S1')\n",
    "            plotFeatureMap(c2_, i, 'C2')\n",
    "            plotFeatureMap(s2_, i, 'S2')\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PartA_Qn1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
