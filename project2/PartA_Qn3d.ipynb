{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"PartA_Qn3d.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"T8dHUN3BdSzM","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5Ym8hIuzRrfA","colab":{"base_uri":"https://localhost:8080/","height":64},"outputId":"c9c79249-9530-40b0-bc13-426278fe55b0","executionInfo":{"status":"ok","timestamp":1573816679023,"user_tz":-480,"elapsed":3071,"user":{"displayName":"Kyle Huang Junyuan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBSuodqmz2v_hKv0jFD1sdMt0OfTPbhLoe_YEJFmBw=s64","userId":"18264821754337088865"}}},"source":["#\n","# Project 2, starter code Part a\n","#\n","\n","import math\n","import tensorflow as tf\n","import numpy as np\n","import pylab as plt\n","import pickle"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TANmKrj_RrfG","colab":{}},"source":["NUM_CLASSES = 10\n","IMG_SIZE = 32\n","NUM_CHANNELS = 3\n","learning_rate = 0.001\n","epochs = 1000\n","batch_size = 128\n","DROPOUT = 0.8\n","\n","seed = 10\n","np.random.seed(seed)\n","tf.set_random_seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CuqIa1_uRrfI","colab":{}},"source":["def load_data(file):\n","    with open(file, 'rb') as fo:\n","        try:\n","            samples = pickle.load(fo)\n","        except UnicodeDecodeError:  #python 3.x\n","            fo.seek(0)\n","            samples = pickle.load(fo, encoding='latin1')\n","\n","    data, labels = samples['data'], samples['labels']\n","\n","    data = np.array(data, dtype=np.float32)\n","    labels = np.array(labels, dtype=np.int32)\n","\n","\n","    labels_ = np.zeros([labels.shape[0], NUM_CLASSES])\n","    labels_[np.arange(labels.shape[0]), labels-1] = 1\n","\n","    return data, labels_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MReWO-qyRrfL","colab":{}},"source":["def cnn(images, c1_filters, c2_filters):\n","    images = tf.reshape(images, [-1, IMG_SIZE, IMG_SIZE, NUM_CHANNELS])\n","\n","    # C1 (50x24x24): Conv layer, 50 filters, window size 9x9, VALID padding, ReLU\n","    # S1 (50x12x12): Max pooling layer, window size 2x2, stride = 2, VALID padding\n","    W1 = tf.Variable(tf.truncated_normal([9, 9, NUM_CHANNELS, c1_filters], stddev=1.0/np.sqrt(NUM_CHANNELS*9*9)), name='weights_1')\n","    b1 = tf.Variable(tf.zeros([c1_filters]), name='biases_1')  \n","    conv_1 = tf.nn.relu(tf.nn.conv2d(images, W1, [1, 1, 1, 1], padding='VALID') + b1)\n","    conv_1 = tf.nn.dropout(conv_1, DROPOUT)\n","    pool_1 = tf.nn.max_pool(conv_1, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_1')\n","\n","    # C2 (60x8x8): Conv layer, 60 filters, window size 5x5, VALID padding, ReLU\n","    # S2 (60x4x4): Max pooling layer, window size 2x2, stride = 2, VALID padding\n","    W2 = tf.Variable(tf.truncated_normal([5, 5, c1_filters, c2_filters], stddev=1.0/np.sqrt(c1_filters*5*5)), name='weights_2')\n","    b2 = tf.Variable(tf.zeros([c2_filters]), name='biases_2')\n","    conv_2 = tf.nn.relu(tf.nn.conv2d(pool_1, W2, [1, 1, 1, 1], padding='VALID') + b2)\n","    conv_2 = tf.nn.dropout(conv_2, DROPOUT)\n","    pool_2 = tf.nn.max_pool(conv_2, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_2')\n","\n","    # Flatten (dim = 60x4x4 = 960)\n","    dim = pool_2.get_shape()[1].value * pool_2.get_shape()[2].value * pool_2.get_shape()[3].value \n","    pool_2_flat = tf.reshape(pool_2, [-1, dim])\n","\n","    # F3: Fully connected layer of size 300 (960 -> 300)\n","    W3 = tf.Variable(tf.truncated_normal([dim, 300], stddev=1.0/np.sqrt(dim)), name='weights_3')\n","    b3 = tf.Variable(tf.zeros([300]), name='biases_3')\n","    f3_logits = tf.matmul(pool_2_flat, W3) + b3\n","    f3_logits = tf.nn.dropout(f3_logits, DROPOUT)\n","\n","    # F4: Softmax layer of size 10 (300 -> 10)\n","    W4 = tf.Variable(tf.truncated_normal([300, NUM_CLASSES], stddev=1.0/np.sqrt(300)), name='weights_4')\n","    b4 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_4')\n","    f4_logits = tf.matmul(f3_logits, W4) + b4\n","\n","    return conv_1, pool_1, conv_2, pool_2, f4_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cbVhB4GYOAp9","colab":{}},"source":["# Main Program\n","if __name__ == '__main__':\n","    trainX, trainY = load_data('/content/drive/My Drive/NTU - Year 3 Sem 1/CZ4042 Neural Network/Project 2/data_batch_1')\n","    print(trainX.shape, trainY.shape)\n","\n","    testX, testY = load_data('/content/drive/My Drive/NTU - Year 3 Sem 1/CZ4042 Neural Network/Project 2/test_batch_trim')\n","    print(testX.shape, testY.shape)\n","\n","    # Scaling the train & test inputs\n","    trainX = (trainX - np.min(trainX, axis = 0))/np.max(trainX, axis = 0)\n","    testX = (testX - np.min(testX, axis = 0))/np.max(testX, axis = 0)\n","\n","    # Create the model\n","    x = tf.placeholder(tf.float32, [None, NUM_CHANNELS*IMG_SIZE*IMG_SIZE]) # 3x32x32\n","    y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n","\n","    c1, s1, c2, s2, logits = cnn(x, 310, 210)\n","\n","    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits)\n","    loss = tf.reduce_mean(cross_entropy)\n","\n","    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n","\n","    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n","    correct_prediction = tf.cast(correct_prediction, tf.float32)\n","    accuracy = tf.reduce_mean(correct_prediction)\n","\n","    N = len(trainX)\n","    idx = np.arange(N)\n","\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        train_loss = []\n","        test_acc = []\n","\n","        for e in range(epochs):\n","            np.random.shuffle(idx)\n","            trainX, trainY = trainX[idx], trainY[idx]\n","            temp_train_loss = []\n","\n","            for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):\n","                train_step.run(feed_dict={x: trainX[start:end], y_: trainY[start:end]})\n","                temp_train_loss.append(loss.eval(feed_dict={x: trainX[start:end], y_: trainY[start:end]}))\n","\n","            # _, loss_ = sess.run([train_step, loss], {x: trainX, y_: trainY})\n","\n","            trainLoss = np.mean(np.array(temp_train_loss))\n","            testAcc = accuracy.eval(feed_dict={x: testX, y_: testY})\n","            train_loss.append(trainLoss)\n","            test_acc.append(testAcc)\n","\n","            if (True): # e % 10 == 0 or e == epochs-1\n","                print('Epoch: ', e, ' | Train loss: ', trainLoss, ' | Test acc: ', testAcc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e-jpJ6PxWR_b","colab":{}},"source":["        # Plot train loss vs test acc\n","        plt.figure(figsize=(10, 8), dpi=100)\n","        plt.suptitle('Adding dropout to the layers')\n","        plt.plot(range(epochs), train_loss, label='Train Loss')\n","        plt.plot(range(epochs), test_acc, label='Test Accuracy')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Accuracy/loss')\n","        plt.legend()\n","        plt.savefig('./3d.Dropout_TrainLoss_TestAcc.png')\n","        plt.show()"],"execution_count":0,"outputs":[]}]}